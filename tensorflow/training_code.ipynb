{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:No traceback has been produced, nothing to debug.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%debug\n",
    "\n",
    "import os, glob\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.rc('image', cmap='inferno')\n",
    "\n",
    "import models.model_2d as mod\n",
    "import forward_model_tf as fm\n",
    "import utils as ut\n",
    "\n",
    "# for resizing PSFs as appropriate:\n",
    "import cv2\n",
    "\n",
    "import load_PSFs\n",
    "\n",
    "import sys\n",
    "# sys.path.append(\"../data/denoising_experiments/\")\n",
    "sys.path.append(\"../common/\")\n",
    "import denoising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /home/dshteinbok/anaconda3/envs/tf/lib/python3.9/site-packages/cv2/../../../../lib/libtinfo.so.6: no version information available (required by /bin/bash)\r\n",
      "/bin/bash: gpustat: command not found\r\n"
     ]
    }
   ],
   "source": [
    "!gpustat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training code for 2D spatially-varying deconvolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one setting to rule them all\n",
    "# preset = 'single_depth_probe'\n",
    "# preset = 'single_depth_probe_noisy'\n",
    "# preset = 'single_depth_probe_noisy_11'\n",
    "# preset = 'multiple_depths_probe'\n",
    "# preset = 'probe_noisy_11_unshifted'\n",
    "# preset = 'probe_noisy_11_unshifted_crazy'\n",
    "# preset = 'probe_noisy_unshifted_equivariant'\n",
    "preset = 'noisy_5_test'\n",
    "# preset = 'new_code'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare all the variables that control how the model is set up and trained.\n",
    "# Point to the training data:\n",
    "# target_dir = '../data/TrainingData/Ground_truth_downsampled/'  # path to objects (ground truth)\n",
    "# input_dir = '../data/TrainingData/Simulated_Miniscope_2D_Training_data/'    # path to simulated measurements (inputs to deconv.)\n",
    "\n",
    "target_dir = '../data/nV3_resized/'  # path to objects (ground truth)\n",
    "# input_dir = '../data/nV3_simulated/'    # path to simulated measurements (inputs to deconv.)\n",
    "# input_dir = '../data/nV3_mastermat_bare/'    # path to simulated measurements (inputs to deconv.)\n",
    "input_dir = '../data/nV3_mastermat_probe_unshifted/'    # path to simulated measurements (inputs to deconv.)\n",
    "# input_dir = '../data/nV3_mastermat_probe_unshifted_noisy/'\n",
    "# input_dirs = ['../data/nV3_mastermat_probe_unshifted/']  \n",
    "# input_dirs = ['../data/nV3_mastermat_probe_unshifted_depth1/',\n",
    "#              '../data/nV3_mastermat_probe_unshifted_depth2/',\n",
    "#              '../data/nV3_mastermat_probe_unshifted_depth3/']\n",
    "\n",
    "# target_dir = '../data/TrainingData/Ground_truth_downsampled/'\n",
    "# input_dir = '../data/TrainingData/Simulated_Miniscope_2D_Training_data/'\n",
    "\n",
    "# PSF load type: could be \"matlab\"\n",
    "# psf_loadtype = \"matlab\"\n",
    "psf_loadtype = \"csv\"\n",
    "\n",
    "# PSF locations, if psf_loadtype==\"matlab\"\n",
    "filter_init_path = '../data/multiWienerPSFStack_40z_aligned.mat' # initialize with 9 PSFs\n",
    "filter_key = 'multiWienerPSFStack_40z'  # key to load in\n",
    "\n",
    "# multiple_psfs = False\n",
    "multiple_psfs = True\n",
    "\n",
    "# PSF directory, if psf_loadtype==\"csv\"\n",
    "# psfs_path = '../data/nV3_PSFs'\n",
    "# psfs_path = \"../data/nV3_PSFs_flat_hd\"\n",
    "psfs_path = \"../data/nV3_PSFs_probe_mark_green\"\n",
    "psfs_paths = [\"../data/nV3_PSFs_probe_mark_green\"]\n",
    "# psfs_paths = [\"../data/nV3_PSFs_probe_depth1/\", \"../data/nV3_PSFs_probe_depth2/\", \"../data/nV3_PSFs_probe_depth3/\"]\n",
    "# meta_path = '../data/nV3_PSFs_meta/PSF_Shifts.csv'\n",
    "# meta_path = \"../data/nV3_PSFs_flat_meta/metafile_hd.csv\"\n",
    "meta_path = \"../data/nV3_PSFs_meta/metafile_probe_mark.csv\"\n",
    "meta_paths = [\"../data/nV3_PSFs_probe_depths/metafile_probe_depth1.csv\", \\\n",
    "              \"../data/nV3_PSFs_probe_depths/metafile_probe_depth2.csv\",\n",
    "             \"../data/nV3_PSFs_probe_depths/metafile_probe_depth3.csv\"]\n",
    "\n",
    "# Pixel size of the images we're dealing with. This must be the same as the desired PSF size. \n",
    "# Program will adjust size of PSFs by linear interpolation as needed, but will only crop images.\n",
    "# img_dims is (width, height)\n",
    "img_dims = (1280, 800)\n",
    "# img_dims = (648, 486)\n",
    "\n",
    "# choose network type to train\n",
    "model_type='multiwiener' # choices are 'multiwiener', 'wiener', 'unet'\n",
    "\n",
    "# IMPORTANT! CHANGE WHEN NEEDED!\n",
    "# where to store weights and training info\n",
    "# training_location = \"saved_models/multiwiener_nV3_bare/model_weights\"\n",
    "# epochlog_location = \"saved_models/multiwiener_nV3_bare/epoch.log\"\n",
    "# training_location = \"saved_models/multiwiener_nV3_probe/model_weights\"\n",
    "training_location = \"saved_models/multiwiener_nV3_probe_depths/model_weights\"\n",
    "# epochlog_location = \"saved_models/multiwiener_nV3_probe/epoch.log\"\n",
    "epochlog_location = \"saved_models/multiwiener_nV3_probe_depths/epoch.log\"\n",
    "# training_location = \"saved_models/waller/model_weights\"\n",
    "# epochlog_location = \"saved_models/waller/epoch.log\"\n",
    "\n",
    "# legacy_training decides whether to use the old way of training, with GradientTape etc\n",
    "# or the more standard model.fit() approach.\n",
    "# The former was used by the original Waller code.\n",
    "legacy_training = True\n",
    "smallset = False\n",
    "backup_location = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if preset == 'single_depth_probe':\n",
    "    multiple_psfs = False\n",
    "    target_dir = '../data/nV3_resized/'  # path to objects (ground truth)\n",
    "#     input_dir = '../data/nV3_mastermat_probe_unshifted/'    # path to simulated measurements (inputs to deconv.)\n",
    "    psfs_path = \"../data/nV3_PSFs_probe_mark_green\"\n",
    "    training_location = \"saved_models/multiwiener_nV3_probe/model_weights\"\n",
    "    epochlog_location = \"saved_models/multiwiener_nV3_probe/epoch.log\"\n",
    "if preset == 'single_depth_probe_noisy':\n",
    "    multiple_psfs = False\n",
    "    target_dir = '../data/nV3_resized/'  # path to objects (ground truth)\n",
    "    input_dir = '../data/nV3_mastermat_probe_unshifted/'    # path to simulated measurements (inputs to deconv.)\n",
    "#     input_dir = '../data/nV3_mastermat_probe_unshifted_noisy/'\n",
    "#     input_dir = '../data/nV3_mastermat_probe_unshifted/' # temporary, since we are adding noise on training\n",
    "    psfs_path = \"../data/nV3_PSFs_probe_mark_green\"\n",
    "    training_location = \"saved_models/multiwiener_nV3_probe_noisy2/model_weights\"\n",
    "    epochlog_location = \"saved_models/multiwiener_nV3_probe_noisy2/epoch.log\"\n",
    "    training_noise_sigma=4.2\n",
    "elif preset == 'multiple_depths_probe':\n",
    "    multiple_psfs = True\n",
    "    target_dir = '../data/nV3_resized/'  # path to objects (ground truth)\n",
    "    input_dirs = ['../data/nV3_mastermat_probe_unshifted_depth1/',\n",
    "             '../data/nV3_mastermat_probe_unshifted_depth2/',\n",
    "             '../data/nV3_mastermat_probe_unshifted_depth3/']\n",
    "    psfs_paths = [\"../data/nV3_PSFs_probe_depth1/\", \n",
    "                  \"../data/nV3_PSFs_probe_depth2/\", \n",
    "                  \"../data/nV3_PSFs_probe_depth3/\"]\n",
    "    training_location = \"saved_models/multiwiener_nV3_probe_depths/model_weights\"\n",
    "    epochlog_location = \"saved_models/multiwiener_nV3_probe_depths/epoch.log\"\n",
    "elif preset == 'single_depth_probe_noisy_11':\n",
    "    # noisy with a standard deviation of 11, as observed\n",
    "    multiple_psfs = False\n",
    "    target_dir = '../data/nV3_resized/'  # path to objects (ground truth)\n",
    "    input_dir = '../data/nV3_mastermat_probe_unshifted/'    # path to simulated measurements (inputs to deconv.)\n",
    "#     input_dir = '../data/nV3_mastermat_probe_unshifted_noisy/'\n",
    "#     input_dir = '../data/nV3_mastermat_probe_unshifted/' # temporary, since we are adding noise on training\n",
    "    psfs_path = \"../data/nV3_PSFs_probe_mark_green\"\n",
    "    training_location = \"saved_models/multiwiener_nV3_probe_noisy3/model_weights\"\n",
    "    epochlog_location = \"saved_models/multiwiener_nV3_probe_noisy3/epoch.log\"\n",
    "    training_noise_sigma=11/255\n",
    "elif preset == 'probe_noisy_11_unshifted':\n",
    "    # noisy with a standard deviation of 11, as observed\n",
    "    multiple_psfs = False\n",
    "    target_dir = '../data/nV3_resized/'  # path to objects (ground truth)\n",
    "    input_dir = '../data/nV3_mastermat_probe_undistorted_corrected/'    # path to simulated measurements (inputs to deconv.)\n",
    "#     input_dir = '../data/nV3_mastermat_probe_unshifted_noisy/'\n",
    "#     input_dir = '../data/nV3_mastermat_probe_unshifted/' # temporary, since we are adding noise on training\n",
    "    psfs_path = \"../data/nV3_PSFs_probe_mark_green\"\n",
    "    training_location = \"saved_models/multiwiener_nV3_probe_noisy4/model_weights\"\n",
    "    epochlog_location = \"saved_models/multiwiener_nV3_probe_noisy4/epoch.log\"\n",
    "    training_noise_sigma=11/255\n",
    "elif preset == 'probe_noisy_11_unshifted_crazy':\n",
    "    # noisy with a standard deviation of 11, as observed\n",
    "    multiple_psfs = False\n",
    "    target_dir = '../data/nV3_resized/'  # path to objects (ground truth)\n",
    "    input_dir = '../data/nV3_mastermat_probe_undistorted_corrected/'    # path to simulated measurements (inputs to deconv.)\n",
    "#     input_dir = '../data/nV3_mastermat_probe_unshifted_noisy/'\n",
    "#     input_dir = '../data/nV3_mastermat_probe_unshifted/' # temporary, since we are adding noise on training\n",
    "    psfs_path = \"../data/nV3_PSFs_probe_mark_green\"\n",
    "    training_location = \"saved_models/multiwiener_nV3_probe_noisy5/model_weights\"\n",
    "    epochlog_location = \"saved_models/multiwiener_nV3_probe_noisy5/epoch.log\"\n",
    "    crazy_external_noise_addition = True\n",
    "    training_noise_sigma=11/255\n",
    "    pooling='average'\n",
    "elif preset == 'probe_noisy_unshifted_equivariant':\n",
    "    # noisy with a standard deviation of 11, as observed\n",
    "    multiple_psfs = False\n",
    "    target_dir = '../data/nV3_resized/'  # path to objects (ground truth)\n",
    "    input_dir = '../data/nV3_mastermat_probe_undistorted_corrected/'    # path to simulated measurements (inputs to deconv.)\n",
    "#     input_dir = '../data/nV3_mastermat_probe_unshifted_noisy/'\n",
    "#     input_dir = '../data/nV3_mastermat_probe_unshifted/' # temporary, since we are adding noise on training\n",
    "    psfs_path = \"../data/nV3_PSFs_probe_mark_green\"\n",
    "    training_location = \"saved_models/multiwiener_nV3_probe_equivariant/model_weights\"\n",
    "    epochlog_location = \"saved_models/multiwiener_nV3_probe_equivariant/epoch.log\"\n",
    "#     pooling = 'maxblur'\n",
    "    pooling = 'averageblur'\n",
    "    crazy_external_noise_addition = True\n",
    "    training_noise_sigma=11/255\n",
    "elif preset == 'noisy_5_test':\n",
    "    # noisy with a standard deviation of 11, as observed\n",
    "    multiple_psfs = False\n",
    "    target_dir = '../data/nV3_resized/'  # path to objects (ground truth)\n",
    "    input_dir = '../data/nV3_mastermat_probe_undistorted_corrected/'    # path to simulated measurements (inputs to deconv.)\n",
    "#     input_dir = '../data/nV3_mastermat_probe_unshifted_noisy/'\n",
    "#     input_dir = '../data/nV3_mastermat_probe_unshifted/' # temporary, since we are adding noise on training\n",
    "    psfs_path = \"../data/nV3_PSFs_probe_mark_green\"\n",
    "    training_location = \"saved_models/noisy_5_test/model_weights\"\n",
    "    epochlog_location = \"saved_models/noisy_5_test/epoch.log\"\n",
    "    crazy_external_noise_addition = True\n",
    "    training_noise_sigma=11/255\n",
    "    pooling='average'\n",
    "elif preset == 'new_code':\n",
    "    # noisy with a standard deviation of 11, as observed\n",
    "    multiple_psfs = False\n",
    "    target_dir = '../data/nV3_resized/'  # path to objects (ground truth)\n",
    "    input_dir = '../data/nV3_mastermat_probe_undistorted_corrected/'    # path to simulated measurements (inputs to deconv.)\n",
    "#     input_dir = '../data/nV3_mastermat_probe_unshifted_noisy/'\n",
    "#     input_dir = '../data/nV3_mastermat_probe_unshifted/' # temporary, since we are adding noise on training\n",
    "    psfs_path = \"../data/nV3_PSFs_probe_mark_green\"\n",
    "#     training_location = \"saved_models/multiwiener_nV3_probe_noisy5_new_way/model_weights\"\n",
    "    training_location = \"saved_models/multiwiener_nV3_probe_noisy5_new_way2/model_weights\"\n",
    "#     epochlog_location = \"saved_models/multiwiener_nV3_probe_noisy5/epoch.log\"\n",
    "    crazy_external_noise_addition = True\n",
    "    training_noise_sigma=11/255\n",
    "    pooling='average'\n",
    "    legacy_training = False\n",
    "    smallset=True\n",
    "    backup_location = \"saved_models/multiwiener_nV3_probe_noisy5_new_way2/backup\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiple_psfs = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dirs = [input_dir]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make dataset and dataloader for training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "# batch_size = 1\n",
    "# batch_size = 2\n",
    "batch_size=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22126\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if multiple_psfs:\n",
    "    target_path = (sorted(glob.glob(target_dir + '*')))*len(input_dirs)\n",
    "    input_path_lists = [(sorted(glob.glob(input_dir + '*'))) for input_dir in input_dirs]\n",
    "    input_path = []\n",
    "    for pathset in input_path_lists:\n",
    "        for path in pathset:\n",
    "            input_path.append(path)\n",
    "else:\n",
    "    input_path = sorted(glob.glob(input_dir + '*'))\n",
    "    target_path = sorted(glob.glob(target_dir + '*'))\n",
    "\n",
    "image_count=len(os.listdir(target_dir))\n",
    "print(image_count) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22126"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22126"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(target_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/nV3_resized/9999.png'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_path[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_path_lists[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_path[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16595\n",
      "5531\n",
      "5532\n",
      "1844\n"
     ]
    }
   ],
   "source": [
    "# Create a first dataset of file paths and labels\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_path, target_path))\n",
    "dataset = dataset.shuffle(image_count, seed=0, reshuffle_each_iteration=False)\n",
    "\n",
    "\n",
    "# Split into train/validation\n",
    "if smallset:\n",
    "    val_size = int(5)\n",
    "    train_ds = dataset.skip(val_size).take(20)\n",
    "else:\n",
    "    val_size = int(image_count * 0.25)\n",
    "    # took a small number of samples for test training to make epochs go faster\n",
    "    train_ds = dataset.skip(val_size)\n",
    "#train_ds = dataset.skip(val_size).take(100)\n",
    "\n",
    "    \n",
    "val_ds = dataset.take(val_size)\n",
    "\n",
    "print(tf.data.experimental.cardinality(train_ds).numpy())\n",
    "print(tf.data.experimental.cardinality(val_ds).numpy())\n",
    "\n",
    "train_ds = train_ds.map(ut.parse_function, num_parallel_calls=AUTOTUNE)\n",
    "val_ds = val_ds.map(ut.parse_function, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "train_ds = ut.configure_for_performance(train_ds,batch_size)\n",
    "val_ds = ut.configure_for_performance(val_ds,batch_size)\n",
    "\n",
    "print(tf.data.experimental.cardinality(train_ds).numpy())\n",
    "print(tf.data.experimental.cardinality(val_ds).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x, y in train_ds:\n",
    "#     print(\"x has shape: \", x.shape)\n",
    "#     print(\"y has shape: \", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import imageio\n",
    "# plt.imshow(imageio.imread('../data/nV3_mastermat_bare/1079.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ds.get_single_element?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualzie data to make sure all is good\n",
    "# import pdb\n",
    "# pdb.set_trace()\n",
    "# input_batch, target_batch = next(iter(val_ds))\n",
    "# f, ax = plt.subplots(1, 2, figsize=(15,15))\n",
    "\n",
    "# ax[0].imshow(input_batch[0,:,:,0] + np.random.normal(scale=training_noise_sigma, size=input_batch[0,:,:,0].shape), vmax = 1)\n",
    "# ax[0].set_title('Input Data')\n",
    "\n",
    "# ax[1].imshow(target_batch[0,:,:,0], vmax = 1)\n",
    "# ax[1].set_title('Target Data')\n",
    "\n",
    "# print(input_batch[0,:,:,0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.max(input_batch[0,:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(input_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for path in input_path:\n",
    "#     try:\n",
    "#         print(path)\n",
    "#         imageio.imread(path)\n",
    "#     except:\n",
    "#         print(\"BROKEN: \" + path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(train_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load in Psfs and initialize network to train\n",
    "\n",
    "Here we initialize with 9 PSFs taken from different parts in the field of view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/tmp/ipykernel_3020875/3845168668.py\u001b[0m(30)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     28 \u001b[0;31m\u001b[0;31m# handle multiple sets of PSFs:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     29 \u001b[0;31m    \u001b[0;31m# FIXME: magic number 63 is 3*the number of PSFs per metafile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 30 \u001b[0;31m    \u001b[0;32mif\u001b[0m \u001b[0mmultiple_psfs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     31 \u001b[0;31m        \u001b[0mpsfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_dims\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_dims\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m21\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     32 \u001b[0;31m        \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpsfs_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../common\")\n",
    "import csv_psfs\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "# load the PSFs\n",
    "if psf_loadtype == \"matlab\":\n",
    "    registered_psfs_path = filter_init_path\n",
    "    psfs = scipy.io.loadmat(registered_psfs_path)\n",
    "    psfs=psfs[filter_key]\n",
    "elif psf_loadtype == \"csv\":\n",
    "#     # expect a directory with a bunch of PSFs therein as separate csvs;\n",
    "#     # list out all csv file names in the directory\n",
    "#     psf_paths = glob.glob(psfs_path.removesuffix('/') + '/*')\n",
    "#     # iterate through that list,\n",
    "#     # open and append each to the psfs array,\n",
    "#     psfs = [[]]\n",
    "#     for path in psf_paths:\n",
    "#         #psfs[0].append(np.loadtxt(path, delimiter=',', encoding='utf-8-sig'))\n",
    "#         psfs[0].append(np.loadtxt(path, delimiter=','))\n",
    "#     # convert the psfs array to an np.ndarray\n",
    "#     psfs = np.transpose(np.asarray(psfs), (2,3,1,0))\n",
    "    set_trace()\n",
    "##     psfs = load_PSFs.load_PSFs_csv(psfs_path, meta_path, img_dims)[:,:,:,0]\n",
    "#     if not multiple_psfs: \n",
    "#         psfs = csv_psfs.pad_as_center(csv_psfs.load_from_dir(psfs_path), img_dims[1], img_dims[0])\n",
    "\n",
    "# handle multiple sets of PSFs:\n",
    "    # FIXME: magic number 63 is 3*the number of PSFs per metafile\n",
    "    if multiple_psfs:\n",
    "        psfs = np.empty((img_dims[1], img_dims[0], 21*3))\n",
    "        for i in range(len(psfs_paths)):\n",
    "            psfs[:,:,i:i+21] = csv_psfs.pad_as_center(csv_psfs.load_from_dir(psfs_paths[i]), img_dims[1], img_dims[0])\n",
    "    else:\n",
    "        psfs = csv_psfs.pad_as_center(csv_psfs.load_from_dir(psfs_path), img_dims[1], img_dims[0])\n",
    "else:\n",
    "    raise ValueError(\"Not sure how to load PSFs\")\n",
    "psfs.shape\n",
    "\n",
    "# for testing, cut the psf and look at padding behaviour\n",
    "#psfs = psfs[162:324, 216:432, :, :]\n",
    "#psfs.shape\n",
    "# Test result: it works!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psfs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine the PSFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = 10\n",
    "plt.imshow(psfs[368:432,608:672,ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above, it seems that the PSFs indeed start out centered. This means that something happens during training to cause them to shift around. I wonder if it's possible that the model I've been interrogating was trained on distorted data (that would explain this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(psfs_paths) == list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(\"string\") == str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psfs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_trace()\n",
    "\n",
    "if model_type=='unet':\n",
    "    model =mod.UNet(486, 648, \n",
    "                             encoding_cs=[24, 64, 128, 256, 512, 1024],\n",
    "                             center_cs=1024,\n",
    "                             decoding_cs=[512, 256, 128, 64, 24, 24],\n",
    "                             skip_connections=[True, True, True, True, True, False])\n",
    "elif model_type=='wiener':\n",
    "\n",
    "    registered_psfs_path = filter_init_path\n",
    "    psfs = scipy.io.loadmat(registered_psfs_path)\n",
    "    psfs=psfs[filter_key]\n",
    "    psfs=psfs[:,:,0,0]\n",
    "    psfs=psfs/np.max(psfs)\n",
    "    \n",
    "    Ks=1\n",
    "\n",
    "    model = mod.UNet_wiener(486, 648, psfs, Ks, \n",
    "                             encoding_cs=[24, 64, 128, 256, 512, 1024],\n",
    "                             center_cs=1024,\n",
    "                             decoding_cs=[512, 256, 128, 64, 24, 24],\n",
    "                             skip_connections=[True, True, True, True, True, False])\n",
    "    \n",
    "    print(psfs.shape, 1)\n",
    "    \n",
    "elif model_type=='multiwiener':\n",
    "    # assume psfs is already 3-dimensional if our PSFs are used\n",
    "    # if matlab-style is used, it's typically an academic who likes 4D\n",
    "    if psf_loadtype == \"matlab\":\n",
    "        psfs=psfs[:,:,:,0]\n",
    "    print(psfs.shape)\n",
    "    #psfs = cv2.resize(psfs, img_dims)\n",
    "    # rather than resizing, zero-pad to appropriate size\n",
    "    # this means that original PSF can be any size, but Image Delta must be set correctly\n",
    "    \n",
    "    print(psfs.shape)\n",
    "    psfs=psfs/np.max(psfs)\n",
    "    if multiple_psfs:\n",
    "        psfs_keep = psfs[:,:,list(range(0,11))+list(range(21,32))+list(range(43,54))]*1\n",
    "#         psfs_keep = psfs*1\n",
    "    #     psfs_keep = psfs[:,:,list(range(0,11))+list(range(21,32))]*1\n",
    "    #     psfs_keep = psfs[:,:,0:21]*1\n",
    "        print(psfs_keep.shape)\n",
    "\n",
    "    #     Ks =np.ones((1,1,psfs.shape[2]))\n",
    "        Ks_keep = np.ones((1,1,psfs_keep.shape[2]))\n",
    "        print(Ks_keep.shape)\n",
    "\n",
    "        model =mod.UNet_multiwiener_resize(img_dims[1], img_dims[0], psfs_keep, Ks_keep, \n",
    "                             encoding_cs=[24, 64, 128, 256, 512, 1024],\n",
    "                             center_cs=1024,\n",
    "                             decoding_cs=[512, 256, 128, 64, 24, 24],\n",
    "                             skip_connections=[True, True, True, True, True, False], psfs_trainable=True)\n",
    "#     model =mod.UNet_multiwiener_resize(img_dims[1], img_dims[0], psfs, Ks, \n",
    "#                          encoding_cs=[24, 64, 128, 256, 512, 1024],\n",
    "#                          center_cs=1024,\n",
    "#                          decoding_cs=[512, 256, 128, 64, 24, 24],\n",
    "#                          skip_connections=[True, True, True, True, True, False])\n",
    "\n",
    "# made PSFs untrainable\n",
    "    else:\n",
    "        Ks =np.ones((1,1,psfs.shape[2]))\n",
    "        model =mod.UNet_multiwiener_resize(img_dims[1], img_dims[0], psfs, Ks, \n",
    "                             encoding_cs=[24, 64, 128, 256, 512, 1024],\n",
    "                             center_cs=1024,\n",
    "                             decoding_cs=[512, 256, 128, 64, 24, 24],\n",
    "                             skip_connections=[True, True, True, True, True, False], psfs_trainable=True,\n",
    "                                training_noise=True,\n",
    "                                training_noise_sigma=training_noise_sigma,\n",
    "                                pooling=pooling)\n",
    "#     model =mod.UNet_multiwiener_resize(img_dims[1], img_dims[0], psfs_keep, Ks_keep, \n",
    "#                          encoding_cs=[24, 64, 128, 256, 512, 1024],\n",
    "#                          center_cs=1024,\n",
    "#                          decoding_cs=[512, 256, 128, 64, 24, 24],\n",
    "#                          skip_connections=[True, True, True, True, True, False], psfs_trainable=True)\n",
    "    \n",
    "#     print('initialized filter shape:', psfs.shape, 'initialized K shape:', Ks.shape)\n",
    "# print('initialized filter shape:', psfs_keep.shape, 'initialized K shape:', Ks_keep.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psfs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if legacy_training:\n",
    "    model.build((None, img_dims[1], img_dims[0], 1))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.any(np.isnan(psfs_keep))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indicator_reshaped = tf.convert_to_tensor(np.load(\"../data/fov_indicator.npy\").reshape((1,800,1280,1)), dtype=tf.float32)\n",
    "# indicator_reshaped = tf.convert_to_tensor(np.load(\"../data/fov_indicator.npy\"), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ds.unbatch?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADDED BY DANIEL:\n",
    "# Prepare EpochLogger and load weights, if necessary, into the model\n",
    "%debug\n",
    "set_trace()\n",
    "import epochlog.epochlog as el\n",
    "print(\"done all the crap\")\n",
    "\n",
    "## Training with TF.Dataset\n",
    "initial_learning_rate = 1e-4\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=initial_learning_rate, beta_1=0.9, beta_2=0.999, amsgrad=False) #1e-3 diverges\n",
    "\n",
    "# what's the expected input shape for the model?\n",
    "exp_shape = model.get_config()[\"layers\"][0][\"config\"][\"batch_input_shape\"]\n",
    "print(exp_shape)\n",
    "\n",
    "# Keep results for plotting\n",
    "train_loss_results = []\n",
    "train_accuracy_results = []\n",
    "validtate_loss_results=[]\n",
    "num_epochs = 100\n",
    "loss_func=ut.SSIMLoss_l1_indicator\n",
    "learning_rate_counter=0\n",
    "#for epoch in range(num_epochs):\n",
    "if legacy_training:\n",
    "    epochlogger = el.EpochLogger(model, epochlog_location, training_location)\n",
    "    epochlogger.load_weights()\n",
    "    starting_epoch = epochlogger.epochs_done()\n",
    "    print(\"Starting on epoch number: \" + str(starting_epoch))\n",
    "    for epoch in range(starting_epoch, num_epochs):\n",
    "        validation_loss_avg=tf.keras.metrics.Mean()\n",
    "        epoch_loss_avg = tf.keras.metrics.Mean()\n",
    "        epoch_accuracy = tf.keras.metrics.MeanSquaredError()\n",
    "\n",
    "        # Training loop\n",
    "        iter_num=0\n",
    "        for x, y in train_ds:\n",
    "            # Optimize the model\n",
    "            #print(\"x shape: \" + str(x.shape))\n",
    "            #print(\"y shape: \" + str(y.shape))\n",
    "            # NOTICE: x.shape = y.shape = (2, 486, 648, 1)\n",
    "            # TODO: resize x and y based on expected dimensions for the model\n",
    "            # Crop to the top left corner of the image to make it fit our size if it is too large\n",
    "            if x.shape[1] > exp_shape[1]:\n",
    "                x = x[:,:exp_shape[1],:,:]\n",
    "            if x.shape[2] > exp_shape[2]:\n",
    "                x = x[:,:,:exp_shape[2],:]\n",
    "            if y.shape[1] > exp_shape[1]:\n",
    "                y = y[:,:exp_shape[1],:,:]\n",
    "            if y.shape[2] > exp_shape[2]:\n",
    "                y = y[:,:,:exp_shape[2],:]\n",
    "    #         loss_value, grads = ut.grad(model,loss_func, x, y)\n",
    "    #         loss_value, grads = ut.grad_universal(model,loss_func, x, y)\n",
    "            if crazy_external_noise_addition:\n",
    "                xy_shifted_arr = denoising.rescale_to_one(\n",
    "                    np.asarray([x + np.random.normal(scale=training_noise_sigma, size=x.shape),y]))\n",
    "                loss_value, grads = ut.grad_universal(model,loss_func, xy_shifted_arr[0], xy_shifted_arr[1].astype(np.float32), \n",
    "                                                      indicator_reshaped, training=False)\n",
    "            else:\n",
    "                loss_value, grads = ut.grad_universal(model,loss_func, x, y, indicator_reshaped)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "            # Track progress\n",
    "            epoch_loss_avg.update_state(loss_value)  # Add current batch loss\n",
    "\n",
    "            epoch_accuracy.update_state(y, model(x)) \n",
    "            # Print every 1\n",
    "            if iter_num % 1 == 0:\n",
    "                print(\"Epoch {:03d}: Step: {:03d}, Loss: {:.3f}, MSE: {:.3}\".format(epoch, iter_num,epoch_loss_avg.result(),\n",
    "                                                                            epoch_accuracy.result()),end='\\r')\n",
    "            iter_num=iter_num+1\n",
    "\n",
    "\n",
    "\n",
    "      # End epoch\n",
    "        #print(\"Ending Epoch\")\n",
    "        train_loss_results.append(epoch_loss_avg.result())\n",
    "        train_accuracy_results.append(epoch_accuracy.result())\n",
    "\n",
    "\n",
    "        # skip validation, since SSIM function is broken\n",
    "        for x_val, y_val in val_ds:\n",
    "            val_loss_value= loss_func(model, x_val, y_val, indicator_reshaped)\n",
    "            validation_loss_avg.update_state(val_loss_value)\n",
    "\n",
    "\n",
    "        validtate_loss_results.append(validation_loss_avg.result())    \n",
    "        #if epoch % 1 == 0:\n",
    "            #print(\"Epoch {:03d}: MSE: {:.3}, Training Loss: {:.3f}, Validation Loss: {:.3f}\".format(epoch,\n",
    "            #                                                            epoch_accuracy.result(), epoch_loss_avg.result(), \n",
    "            #                                                                                        validation_loss_avg.result()))\n",
    "        epochlogger.done_epoch()\n",
    "else:\n",
    "#     loss_func=ut.SSIMLoss_l1_indicator_generator(indicator_reshaped, training=False)\n",
    "#     loss_func=ut.SSIMLoss_l1_generator(training=False)\n",
    "    loss_func = ut.SSIMLoss_l1_indicator_Class(indicator_reshaped)\n",
    "#     loss_func=ut.Loss_l1_indicator_generator(indicator_reshaped, training=False)\n",
    "    # load the dataset here? Necessary to have separate x and y datasets because that's how fit() takes it\n",
    "    \n",
    "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=training_location,\n",
    "    save_weights_only=True,\n",
    "    verbose=1)\n",
    "    \n",
    "    if backup_location is None:\n",
    "        backup_restore_callback = tf.keras.callbacks.BackupAndRestore(backup_dir=training_location)\n",
    "    else:\n",
    "        backup_restore_callback = tf.keras.callbacks.BackupAndRestore(backup_dir=backup_location)\n",
    "#     model_checkpoint_callback\n",
    "    \n",
    "    model.compile(optimizer, loss=loss_func)\n",
    "#     print(model_checkpoint_callback.best)\n",
    "\n",
    "#     model.load_weights(training_location)\n",
    "    # weirdly, model keeps restarting its training from the first epoch\n",
    "    # made the modification below based on user30012's answer at\n",
    "    # https://stackoverflow.com/questions/45393429/keras-how-to-save-model-and-continue-training\n",
    "#     score = model.evaluate(val_ds)\n",
    "#     model_checkpoint_callback.best = score\n",
    "    \n",
    "#     epochlogger = el.BaseEpochLogger(model, epochlog_location, training_location)\n",
    "#     epochlogger.load_weights()\n",
    "    \n",
    "#     print(model_checkpoint_callback.best)\n",
    "    model.fit(train_ds, epochs=num_epochs, callbacks=[model_checkpoint_callback, backup_restore_callback], \n",
    "             validation_data=val_ds, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if legacy_training:\n",
    "    import epochlog.epochlog as el\n",
    "    epochlogger = el.EpochLogger(model, epochlog_location, training_location)\n",
    "    epochlogger.load_weights()\n",
    "else:\n",
    "    model.load_weights(training_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_weights('./saved_models/multiwiener')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "plt.imshow(imageio.imread(\"../data/noisy_real_images/10.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_batch, target_batch = next(iter(val_ds))\n",
    "input_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test on validation data\n",
    "input_batch, target_batch = next(iter(val_ds))\n",
    "imnum=0\n",
    "\n",
    "# noisy_input = input_batch + np.random.normal(scale=training_noise_sigma, size=input_batch.shape)\n",
    "noisy_input = input_batch\n",
    "f, ax = plt.subplots(3, 1, figsize=(15,15))\n",
    "# ax[0].imshow((target_batch[imnum,:,:,0]))\n",
    "ax[0].imshow((target_batch[imnum,:,:]))\n",
    "ax[0].set_title('Target Data')\n",
    "\n",
    "# test=model(input_batch[imnum,:,:,0].numpy().reshape((1,486, 648,1)))\n",
    "test=model(noisy_input[imnum,:,:].numpy().reshape((1,img_dims[1], img_dims[0],1)))\n",
    "ax[2].set_title('recon')\n",
    "ax[2].imshow(test[0,:,:1200])\n",
    "\n",
    "ax[1].imshow((noisy_input[imnum,:,:]))\n",
    "ax[1].set_title('Input Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.all(np.isnan(test[0,:,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_reconstruct_index = 9\n",
    "# folderpath = \"../data/real_images/\"\n",
    "# to_reconstruct = imageio.imread(folderpath+\"image_\"+str(to_reconstruct_index)+\".png\")\n",
    "# img_path_to_do = \"../data/denoised_real_images/30.png\"\n",
    "img_path_to_do = \"../data/noisy_real_images/40.png\"\n",
    "to_reconstruct = cv2.resize(imageio.imread(img_path_to_do), (1280, 800))\n",
    "\n",
    "fig, axs = plt.subplots(1,2, figsize=(15,15))\n",
    "axs[0].imshow(to_reconstruct)\n",
    "axs[1].imshow(model(to_reconstruct.reshape((1,800,1280,1)))[0,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(80,128))\n",
    "plt.imshow(model(to_reconstruct.reshape((1,800,1280,1)))[0,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(imageio.imread(\"../data/real_images_unpadded/isxd_9.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once training is working, save your model using: \n",
    "\n",
    "    model.save_weights('./saved_models/model_name')\n",
    "\n",
    "You can save after training is complete, or periodically throughout epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the first PSF as an example of what we're looking for\n",
    "plt.imshow(psfs[:,:,0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
