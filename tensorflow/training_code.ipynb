{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, glob\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.rc('image', cmap='inferno')\n",
    "\n",
    "import models.model_2d as mod\n",
    "import forward_model as fm\n",
    "import utils as ut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gpustat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training code for 2D spatially-varying deconvolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make dataset and dataloader for training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "batch_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dir = '/home/kyrollos/LearnedMiniscope3D/Data/Target/'  # path to objects (ground truth)\n",
    "input_dir = '/home/kyrollos/LearnedMiniscope3D/Data/Train/'    # path to simulated measurements (inputs to deconv.)\n",
    "\n",
    "target_path = sorted(glob.glob(target_dir + '*'))\n",
    "input_path = sorted(glob.glob(input_dir + '*'))\n",
    "\n",
    "image_count=len(os.listdir(target_dir))\n",
    "print(image_count) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a first dataset of file paths and labels\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_path, target_path))\n",
    "dataset = dataset.shuffle(image_count, reshuffle_each_iteration=False)\n",
    "\n",
    "\n",
    "# Split into train/validation\n",
    "val_size = int(image_count * 0.25)\n",
    "train_ds = dataset.skip(val_size)\n",
    "val_ds = dataset.take(val_size)\n",
    "\n",
    "print(tf.data.experimental.cardinality(train_ds).numpy())\n",
    "print(tf.data.experimental.cardinality(val_ds).numpy())\n",
    "\n",
    "train_ds = train_ds.map(ut.parse_function, num_parallel_calls=AUTOTUNE)\n",
    "val_ds = val_ds.map(ut.parse_function, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "train_ds = ut.configure_for_performance(train_ds,batch_size)\n",
    "val_ds = ut.configure_for_performance(val_ds,batch_size)\n",
    "\n",
    "print(tf.data.experimental.cardinality(train_ds).numpy())\n",
    "print(tf.data.experimental.cardinality(val_ds).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualzie data to make sure all is good\n",
    "input_batch, target_batch = next(iter(val_ds))\n",
    "f, ax = plt.subplots(1, 2, figsize=(15,15))\n",
    "\n",
    "ax[0].imshow(input_batch[0,:,:,0], vmax = 1)\n",
    "ax[0].set_title('Input Data')\n",
    "\n",
    "ax[1].imshow(target_batch[0,:,:,0], vmax = 1)\n",
    "ax[1].set_title('Target Data')\n",
    "\n",
    "print(input_batch[0,:,:,0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load in Psfs and initialize network to train\n",
    "\n",
    "Here we initialize with 9 PSFs taken from different parts in the field of view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose network type to train\n",
    "model_type='multiwiener' # choices are 'multiwiener', 'wiener', 'unet'\n",
    "filter_init_path = '../data/multiWienerPSFStack_40z_aligned.mat' # initialize with 9 PSFs\n",
    "filter_key = 'multiWienerPSFStack_40z'  # key to load in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_type=='unet':\n",
    "    model =mod.UNet(486, 648, \n",
    "                             encoding_cs=[24, 64, 128, 256, 512, 1024],\n",
    "                             center_cs=1024,\n",
    "                             decoding_cs=[512, 256, 128, 64, 24, 24],\n",
    "                             skip_connections=[True, True, True, True, True, False])\n",
    "elif model_type=='wiener':\n",
    "\n",
    "    registered_psfs_path = filter_init_path\n",
    "    psfs = scipy.io.loadmat(registered_psfs_path)\n",
    "    psfs=psfs[filter_key]\n",
    "    psfs=psfs[:,:,0,0]\n",
    "    psfs=psfs/np.max(psfs)\n",
    "    \n",
    "    Ks=1\n",
    "\n",
    "    model = mod.UNet_wiener(486, 648, psfs, Ks, \n",
    "                             encoding_cs=[24, 64, 128, 256, 512, 1024],\n",
    "                             center_cs=1024,\n",
    "                             decoding_cs=[512, 256, 128, 64, 24, 24],\n",
    "                             skip_connections=[True, True, True, True, True, False])\n",
    "    \n",
    "    print(psfs.shape, 1)\n",
    "    \n",
    "elif model_type=='multiwiener':\n",
    "    registered_psfs_path = filter_init_path\n",
    "    psfs = scipy.io.loadmat(registered_psfs_path)\n",
    "    psfs=psfs[filter_key]\n",
    "    \n",
    "    psfs=psfs[:,:,:,0]\n",
    "    psfs=psfs/np.max(psfs)\n",
    "    \n",
    "    Ks =np.ones((1,1,9))\n",
    "    \n",
    "    model =mod.UNet_multiwiener_resize(486, 648, psfs, Ks, \n",
    "                         encoding_cs=[24, 64, 128, 256, 512, 1024],\n",
    "                         center_cs=1024,\n",
    "                         decoding_cs=[512, 256, 128, 64, 24, 24],\n",
    "                         skip_connections=[True, True, True, True, True, False])\n",
    "    \n",
    "    print('initialized filter shape:', psfs.shape, 'initialized K shape:', Ks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build((None, 486, 648, 1))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training with TF.Dataset\n",
    "initial_learning_rate = 1e-4\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=initial_learning_rate, beta_1=0.9, beta_2=0.999, amsgrad=False) #1e-3 diverges\n",
    "\n",
    "# Keep results for plotting\n",
    "train_loss_results = []\n",
    "train_accuracy_results = []\n",
    "validtate_loss_results=[]\n",
    "num_epochs = 1000\n",
    "loss_func=ut.SSIMLoss_l1\n",
    "learning_rate_counter=0\n",
    "for epoch in range(num_epochs):\n",
    "    validation_loss_avg=tf.keras.metrics.Mean()\n",
    "    epoch_loss_avg = tf.keras.metrics.Mean()\n",
    "    epoch_accuracy = tf.keras.metrics.MeanSquaredError()\n",
    "\n",
    "    # Training loop\n",
    "    iter_num=0\n",
    "    for x, y in train_ds:\n",
    "        # Optimize the model\n",
    "        loss_value, grads = ut.grad(model,loss_func, x, y)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        # Track progress\n",
    "        epoch_loss_avg.update_state(loss_value)  # Add current batch loss\n",
    "\n",
    "        epoch_accuracy.update_state(y, model(x)) \n",
    "        # Print every 1\n",
    "        if iter_num % 1 == 0:\n",
    "            print(\"Epoch {:03d}: Step: {:03d}, Loss: {:.3f}, MSE: {:.3}\".format(epoch, iter_num,epoch_loss_avg.result(),\n",
    "                                                                        epoch_accuracy.result()),end='\\r')\n",
    "        iter_num=iter_num+1\n",
    "        \n",
    "    \n",
    "\n",
    "  # End epoch\n",
    "    train_loss_results.append(epoch_loss_avg.result())\n",
    "    train_accuracy_results.append(epoch_accuracy.result())\n",
    "\n",
    "\n",
    "    for x_val, y_val in val_ds:\n",
    "        val_loss_value= loss_func(model, x_val, y_val)\n",
    "        validation_loss_avg.update_state(val_loss_value)\n",
    "        \n",
    "        \n",
    "    validtate_loss_results.append(validation_loss_avg.result())    \n",
    "    if epoch % 1 == 0:\n",
    "        print(\"Epoch {:03d}: MSE: {:.3}, Training Loss: {:.3f}, Validation Loss: {:.3f}\".format(epoch,\n",
    "                                                                    epoch_accuracy.result(), epoch_loss_avg.result(), \n",
    "                                                                                                validation_loss_avg.result()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_weights('./saved_models/multiwiener')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test on validation data\n",
    "input_batch, target_batch = next(iter(val_ds))\n",
    "imnum=1\n",
    "f, ax = plt.subplots(1, 2, figsize=(15,15))\n",
    "ax[0].imshow((target_batch[imnum,:,:,0]))\n",
    "ax[0].set_title('Target Data')\n",
    "\n",
    "test=model(input_batch[imnum,:,:,0].numpy().reshape((1,486, 648,1)))\n",
    "ax[1].set_title('recon')\n",
    "ax[1].imshow(test[0,:,:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once training is working, save your model using: \n",
    "\n",
    "    model.save_weights('./saved_models/model_name')\n",
    "\n",
    "You can save after training is complete, or periodically throughout epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eager-latest2",
   "language": "python",
   "name": "homekyrollosanaconda3env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
